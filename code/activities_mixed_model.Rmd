---
title: "GC Mixed Model"
author: "Peter Bonner"
date: "23/06/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	fig.height = 5,
	fig.width = 8,
	message = FALSE,
	warning = FALSE,
	cache = TRUE,
	dpi = 180
)
```


```{r}
library(ggthemr)
library(tidyverse)
library(tidymodels)
library(readr)
library(lme4)
library(corrr)
library(GGally)
ggthemr("fresh")
library(reshape2)

```

# Read in data

**all modelling performed for males only**

```{r}
rides <- read_rds("data/ride_metrics.rds") %>%
  # drop proprietary variables
  select(-week_total_tss,-week_coggan_if,-critical_power_error, -w_prime_error, -gender)
```

Proprietary variables are dropped because they are determined by another 'performance threshold' value that we have no visual of, nor do we know the value. Furthermore it's calculation differs from the `critica_power` variable I am using for this analysis.

# Principle Compomelted_cor <- melt(rides_cor)

ggplot(data = melted_cor, aes(x=term, y = variable, fill = value)) +
         geom_tile() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))nent Analysis

### Correlation matrix

```{r corr-matrix}
rides_cor <- rides %>%
  dplyr::select(where(is.numeric)) %>%
  correlate(use = "pairwise.complete.obs") %>%
  rearrange()




# rides %>%
#   cor(y = critical_power, x = age, use = "everything", method = "pearson")
```


```{r corr-matrix-plot}
melted_cor <- melt(rides_cor)

ggplot(data = melted_cor, aes(x=term, y = variable, fill = value)) +
         geom_tile() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
```


Correlation matrix suggests that there are several key correlations to look at:

* Week_total_work
* Week_coggan_np
* Week_relative_intensity

Which can be broadly summarised as:
* Those who cycle more have a higher cp
* Those who hold a higher average/NP during sessions have a higher CP
* Those who hold a relatively lower relative intensity have a higher critical power


### Pairwise plot matrix
```{r}
# rides %>%
#   select(gender, week_total_work,week_coggan_np, week_relative_intensity) %>%
#   GGally::ggpairs(aes(colour = gender))
```

### Principle component analysis

We’ll use the recipes package from tidymodels to perform a principal component analysis (PCA).

First, we’ll also use a few recipe steps to preprocess the data for PCA; namely, we need to:

* remove any NA values,
* center all predictors, and
* scale all predictors.


```{r pca-recipe}
pca_df <- rides %>%
  select(-year,-training_phase)

pca_recipe <-
  recipe(~., data = pca_df) %>%
  update_role(id,cp_quantile, new_role = "id") %>%
  step_naomit(all_predictors()) %>%
  step_normalize(all_predictors()) %>%
  step_pca(all_predictors(), id = "pca")

pca_prep <- prep(pca_recipe)

rides_pca <- pca_prep %>%
  tidy(id = "pca")


```

Let’s walk through the steps in this recipe.

* First, we must tell the `recipe()` what’s going on with our model (notice the formula with no outcome) and what data we are using.
* Next, we update the role for cocktail name and category, since these are variables we want to keep around for convenience as identifiers for rows but are not a predictor or outcome.
We need to center and scale the numeric predictors, because we are about to implement PCA.
* Finally, we use `step_pca()` for the actual principal component analysis.
Before using `prep()` these steps have been defined but not actually run or implemented. The prep() function is where everything gets evaluated.

Once we have that done, we can both explore the results of the PCA. Let’s start with checking out how the PCA turned out. We can `tidy()` any of our recipe steps, including the PCA step, which is the second step. Then let’s make a visualization to see what the components look like.


Apply the `recipes::tidy()` method to the output from `recipes::step_pca()` to examine how much variance each component accounts for:


```{r}
pca_prep %>% 
  tidy(id = "pca", type = "variance") %>% 
  dplyr::filter(terms == "percent variance") %>%
  ggplot(aes(x = component, y = value)) + 
  geom_col() +
  scale_y_continuous(limits = c(0,30),
                     breaks = seq(0,30, by = 5)) +
  xlim(c(0, 5)) + 
  ylab("% of total variance")

ggsave(filename = "pca_perc_variance.png", path = "plots")
```


```{r pca-percent-variance-table}
pca_prep %>% 
  tidy(id = "pca", type = "variance") %>% 
  dplyr::filter(terms == "percent variance") %>%
  group_by(component) %>%
  summarise(value = value) %>%
  ungroup() %>%
  mutate(cum_sum = cumsum(value))
  
```


```{r scree-plot}
pca_prep %>% 
  tidy(id = "pca", type = "variance") %>% 
  dplyr::filter(terms == "percent variance") %>%
  ggplot() + 
  geom_line(aes(x = component, y = value)) +
  geom_point(aes(x = component, y = value)) +
  geom_line(aes(x = component, y = cumsum(value))) +
  geom_point(aes(x = component, y = cumsum(value))) +
  scale_y_continuous(limits = c(0,100),
                     breaks = seq(0,100, by = 10)) +
  xlim(c(0, 10)) + 
  ylab("% of total variance")
```

### Plot PCA loadings

```{r view-components}
ggthemr_reset()

# 10,000ft
rides_pca %>% 
  filter(component %in% paste0("PC", 1:4)) %>%
  mutate(component = fct_inorder(component)) %>%
  ggplot(aes(terms, value, fill = terms)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  theme(axis.text.x = element_blank(), 
        axis.ticks.x = element_blank(),
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank()) + 
  labs(x = NULL,
    y = "Relative importance in each principal component") +
  facet_wrap(~ component, ncol = 5) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5),         
        axis.ticks.x = element_blank())

```

Biggest difference in PC1 is `week_total_work`/ `week_total_distance` and `week_relative_intensity`. Training is not likely to have both, the more you train the easier it needs to be. 

Let's zoom into the first four components to better understand which training metrics contribute to positive and negative directions

```{r view-component-contribution}
ggthemr("fresh")

rides_pca %>%
  filter(component %in% paste0("PC", 1:6)) %>%
  mutate(terms = tidytext::reorder_within(terms, 
                                          abs(value), 
                                          component)) %>%
  ggplot(aes(abs(value), terms, fill = value > 0)) +
  geom_col() +
  facet_wrap(~component, scales = "free_y") +
  tidytext::scale_y_reordered() +
  labs(
    x = "Absolute value of contribution",
    y = NULL, fill = "Positive?"
  ) 


```


PC1 is all about how much cycling a person does (work, distance, time). This explains the most variation in the data. PC2 is looking at how hard a person trains (average power, normalised power and relative intensity).


### Plot PCA loadings + scores

How are critical power values distributed in the plane of the first two components?

We have the PCA loadings in `rides_pca`. But we need them in a wide format now for plotting.

```{r}
# get pca loadings into wider format - but reduce number of variables to help plotting

pca_wider <- rides_pca  %>%
  tidyr::pivot_wider(names_from = component, id_cols = terms)
```

```{r}
# 
# define arrow style
# arrow_style <- arrow(length = unit(.05, "inches"),
#                      type = "closed")


pca_plot <-
  juice(pca_prep) %>%
  ggplot(aes(PC1, PC2, colour = as.factor(cp_quantile), shape = gender)) +
  geom_point(alpha = 0.6, 
             size = 2) +
  labs(colour = "CP Quantile",
       shape = "Gender") +
  scale_colour_ggthemr_d()

pca_plot

ggsave(filename = "pca_plot.png", path = "plots")
# pca_plot +
#   geom_segment(data = pca_wider,
#                aes(xend = PC1, yend = PC2), 
#                x = 0, 
#                y = 0, 
#                arrow = arrow_style) + 
#   geom_text(data = pca_wider,
#             aes(x = PC1, y = PC2, label = terms), 
#             hjust = 0, 
#             vjust = 1,
#             size = 5)
```

The `pca_plot` shows:
* quantiles 3 and 4 are to the left;
* quantiles 1 and 2 are to the right.

### Plot variables highlighted by PCA
Letsw view the relationship between critical power quantiles and training volume/intensity

```{r volume + intensity}
# quartiles volume vs intensity
ggplot(rides,
       aes(x = week_total_workout_time, y = week_relative_intensity, shape = gender)) +
         geom_point(aes(colour = as.factor(cp_quantile)),alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE, aes(linetype = gender)) +
  scale_colour_ggthemr_d() +
  labs(colour = "CP Quantile")

ggsave(filename = "vol_vs_intens.png", path = "plots")

# relative intensity vs critical power
ggplot(rides,
       aes(x = week_relative_intensity, y = critical_power, colour = gender)) +
         geom_point(alpha = 0.4) +
  geom_smooth(method = "lm",aes(fill = gender)) +
  scale_colour_ggthemr_d()

ggsave(filename = "relintens_vs_cp.png", path = "plots")

```

The trends are more difficult to see when using average power or normalised power, as someone who is 'fitter' will be able to hold a higher power output than someone who is less fit. However, if we look at relative intensity, we can see that better athletes (quantile 4) tend to spend more time cycling at a lower relative intensity than those who in quantiles 1 & 2.


```{r}
library(cowplot)

rel_intensity_his <- 
  ggplot(rides, aes(x = week_relative_intensity, fill = as.factor(cp_quantile))) +
         geom_histogram(alpha = 0.6) +
         labs(x = "Relative exercise intensity",
              y = "Frequency",
              title = "Do better athletes train at a lower exericse intensity?") +
  facet_wrap(~gender, scales = "free_y") +
  theme(legend.position = "none")

work_hist <-
  ggplot(rides, aes(x = week_total_work, fill = as.factor(cp_quantile))) +
         geom_histogram(alpha = 0.6) +
         labs(x = "Work done per week (J)",
              y = "Frequency",
              title = "Do better athletes do more work per week?",
              fill = "CP quantile") +
  facet_wrap(~gender, scales = "free_y")


plot_grid(rel_intensity_his, work_hist, labels = c("A", "B"), label_size = 12)
```

Comparing the distributions of exercise intensity and work done per week suggests that those who have a higher critical power (quantile 4) perform more work per week and tends to be done at a lower exercise intensity compared to users with a lower critical power value.


```{r remove-unused-dataframes}
remove(arrow_style, pca_plot, pca_prep, pca_recipe, pca_wider, rides_pre_pca, rides_cor,rides_pca, tidied_pca)
```


# Mixed Model

### Preliminaries
```{r}
gc()
```


### Distribution of response variable (Critical Power)

Distribution described with `describe_distribution()`:

Lüdecke D, Ben-Shachar M, Patil I, Makowski D (2020). “Extracting,
Computing and Exploring the Parameters of Statistical Models using R.”
_Journal of Open Source Software_, *5*(53), 2445. doi: 10.21105/joss.02445
(URL: https://doi.org/10.21105/joss.02445).

```{r}
# remove unwanted variables

# pre_mm_df <- rides %>%
#   select(-w_prime, -cp_quantile)

ggplot(rides,
       aes(x = critical_power)) +
  geom_histogram(binwidth = 5)

describe_distribution(pre_mm_df$critical_power)

citation("parameters")
```

Critical power is normally distributed so it does not need any transformations.

### Variable collinearity

The `corvif` function used to determine variable inflations scores is from:

Mixed effects models and extensions in ecology with R. (2009).
Zuur, AF, Ieno, EN, Walker, N, Saveliev, AA, and Smith, GM. Springer.


```{r corvif-function}
#VIF FUNCTION.
#To use:  corvif(YourDataFile)
corvif <- function(dataz) {
  dataz <- as.data.frame(dataz)
  #correlation part
  #cat("Correlations of the variables\n\n")
  #tmp_cor <- cor(dataz,use="complete.obs")
  #print(tmp_cor)
  
  #vif part
  form    <- formula(paste("fooy ~ ",paste(strsplit(names(dataz)," "),collapse=" + ")))
  dataz   <- data.frame(fooy=1,dataz)
  lm_mod  <- lm(form,dataz)
  
  cat("\n\nVariance inflation factors\n\n")
  print(myvif(lm_mod))
}

#Support function for corvif. Will not be called by the user
myvif <- function(mod) {
  v <- vcov(mod)
  assign <- attributes(model.matrix(mod))$assign
  if (names(coefficients(mod)[1]) == "(Intercept)") {
    v <- v[-1, -1]
    assign <- assign[-1]
  } else warning("No intercept: vifs may not be sensible.")
  terms <- labels(terms(mod))
  n.terms <- length(terms)
  if (n.terms < 2) stop("The model contains fewer than 2 terms")
  if (length(assign) > dim(v)[1] ) {
    diag(tmp_cor)<-0
    if (any(tmp_cor==1.0)){
      return("Sample size is too small, 100% collinearity is present")
    } else {
      return("Sample size is too small")
    }
  }
  R <- cov2cor(v)
  detR <- det(R)
  result <- matrix(0, n.terms, 3)
  rownames(result) <- terms
  colnames(result) <- c("GVIF", "Df", "GVIF^(1/2Df)")
  for (term in 1:n.terms) {
    subs <- which(assign == term)
    result[term, 1] <- det(as.matrix(R[subs, subs])) * det(as.matrix(R[-subs, -subs])) / detR
    result[term, 2] <- length(subs)
  }
  if (all(result[, 2] == 1)) {
    result <- data.frame(GVIF=result[, 1])
  } else {
    result[, 3] <- result[, 1]^(1/(2 * result[, 2]))
  }
  invisible(result)
}
#END VIF FUNCTIONS
```


```{r run-corvif}
# Run corvif on rides

rides_vif <- rides %>%
  select(-id, -cp_quantile,-critical_power)

# corvif output
ride_corvif <- corvif(rides_vif)

# create dataframe for plotting
corvif_res <- ride_corvif %>%
  as.data.frame() %>%
  rownames_to_column()
  
```

```{r corvif-plot}

corvif_res %>%
  # arrange by VGIF value. This sorts the dataframe but not the factor levels
  arrange(GVIF) %>%
  # Trick to update factor levels
  mutate(rowname = factor(rowname, levels = rowname)) %>%
  ggplot(aes(x = rowname, y = GVIF)) +
  geom_col() +
  geom_hline(yintercept = 10, linetype = "dashed") +
  coord_flip() +
  labs(x = NULL,
       y = "VIF",
       title = "Variable inflation factor of GCOD variables",
       caption = "VIF > 10 is threshold for variable removal")

ggsave(path = "plots", filename = "rides_vif.png")

```

I need to remove the variables that have VIF values > 10 as they have high collinearity and i also need to provide numeric user ID so that `id` is a factor for the mixed model.

I also need to remove W' and cp_quantile from the model data set they are both functions of the outcome variable (`critical_power`).

## Build a model - Non PCA Data

It's good practice to standardise the explanatory variables so that they have a mean of zero ("centering") and a standard deviation of one ("scaling"). This ensures that the estimated coefficients are all on the same scale, making it easier to compare effect sizes.

```{r create-model-df}

library(afex)

model_df <- rides %>%
  select(-week_total_distance, -week_total_work, -week_average_power, -w_prime, -cp_quantile) %>%
  # make id a factor variable
  group_by(id) %>%
  mutate(id = as.factor(id)) %>%
  ungroup() %>%
  # recode gender as numeric factor for modelling
  mutate(gender = fct_recode(gender, "0" = "M",
                             "1" = "F"),
         gender = as.factor(gender)) %>%
  # scale variables
  mutate_at(vars("week_total_workout_time":"rides_per_training_phase"), scale)
```


**Check whether a significant effect for year exists**

Although we're not interesting in it as a predictor, if there is a significant effect we want to account for it as a random effect in our model.


```{r}
library(beeswarm)

lm_year <- lm(critical_power ~ year, data = model_df)

anova(lm_year)
```

Yes, there is a statistically significant effect of year.

##### Train and testing data
```{r split-data}

library(caret)
# Set the random number stream using `set.seed()` so that the results can be 
# reproduced later. 

set.seed(123)

# Save the split information for an 80/20 split of the data

# creating indices
trainIndex <- createDataPartition(model_df$critical_power,p=0.80,list=FALSE)

#splitting data into training/testing data using the trainIndex object
model_train <- model_df[trainIndex,] #training data (80% of data)
 
model_test <- model_df[-trainIndex,] #testing data (20% of data)

# model_df_split <- initial_split(model_df, prop = 0.8)
```


```{r train-test-sets}
# mixed_model_train <- training(model_df_split)
# mixed_model_test <- testing(model_df_split)
```


##### Create the mixed model - backwards stepwise regression

**Run model**
```{r}
mixed_lmer <- lmer(critical_power ~ . -year -id + (1|year) + (1|id), data = model_train)
```

This model converged, but let's see if we can use an optimiser.

**Search for optimiser package**
```{r}
all_fit(mixed_lmer)
```

**Re-run model with optimiser**

```{r}

mixed_lmer <- lmerTest::lmer(critical_power ~ week_total_elevation_gain + week_average_speed + week_average_cad + week_coggam_variability_index + week_pwr_hr_ratio + age + week_total_workout_time + mean_weekly_rides + week_average_hr + week_coggan_np + week_relative_intensity + rides_per_training_phase + (1|year) + (1|id), data = model_train, REML = FALSE)


options(scipen = 999)  # turn off scientific notation


summary(mixed_lmer)
```

*The random effects part tells you how much variance you find among levels of your grouping factor(s), plus residual variance*.

*The fixed effect part is very similar to a linear model output: intercept & error, slope estimate & error*.

We can see that the variance in id is 173, and is significantly greater than the std deviation (13.2) which means that user id is very important and explains a lot of the variance in critical power, and could be related to a variety of things not captured in the data:
* training history
* genetics
* location
* performance level

We can look at how much user ID explains the variance in critical power by taking the variance for `id` and dividing it by the total variance:

```{r}
173.1/(173.1 + 6.59 + 173.3)

```

So the differences between `id` explains ~ 49% of the variance that's 'left over' *after* the variance explain by our fixed effects.

Once we account for year and user id, there are still MANY statistically significant predictors of critical power. But when looking at the estimate values, it appears to be that `coggan_np` and `week_relative_intensity` look to be important.

Because this is a non-nested design we will need to use the maximum likelihoods methods when comparing models that differ in their fixed effects.

```{r}
remove.packages("lmerTest")

# backward elimination
lmer_step <- lmerTest::step(mixed_lmer)

# show eliminated variables
lmer_step  # 2 ages and week_total_elevation eliminated

# extract model
lmer_final <- lmerTest::get_model(lmer_step)

# show model
summary(lmer_final)

# step_lmer <- stats::step(mixed_lmer)
# 
# step_lmer

```


##### Using `dredge()` to determine the best model

https://www.zoology.ubc.ca/~bio501/R/workshops/modelselection.html

In this process we will:

1. use `dredge()` to determine which linear model best predicts critical power (using AIC as the criterion). `dredge()` carries out an automated model search using subsets of the ‘global’ model provided. Ignore interactions for this exercise. 

2. Determine how many variables are included in the 'best' model.

3. Count the number of models in total having an AIC difference less than or equal to 7. This is one way to decide which models have some support and remain under consideration. For AIC basic principles, see here: https://www.r-bloggers.com/2018/04/how-do-i-interpret-the-aic/

4. Another way to determine the set of models that have support is to use AIC weights. Calculate the Akaike weights of all the models from your dredge() analysis. How much weight is given to the best model**? Are there common features shared among the models having the highest weights?

5. How many models are in the “confidence set” whose cumulative weights reach 0.95***?

6. Use a linear model to fit the “best” model to the data. Produce a summary of the results. Use visreg() to visualize the conditional relationship between bird abundance and each of the three variables in the “best” model one at a time. Visually, which variable seems to have the strongest relationship with bird abundance in the model?

7. Generate an ANOVA table for the best model. Use Type 2 or Type 3 sums of squares so that the order of entry of the main effects in the formula don’t affect the tests (there are no interactions). Why should we view the resulting P-values with a great deal of skepticism****?


```{r}
library(MuMIn)
library(kableExtra)

options(na.action = "na.fail")

# 1. use dredge to find the best model

model_dredge <- dredge(mixed_lmer)

# 3. models with delta AICc < 7
sum(model_dredge$delta < 10)

# 4. calculate AIC weights - another way to determine which models have support
w <- Weights(model_dredge)

w

# 5. models in the 95% "confidence set"

length(cumsum(w)[cumsum(w)< 0.95]) + 1


# 6. extract best models

## models with delta less than 4 and fitted with ML
cand_models <- get.models(model_dredge, subset = delta <4)

# 7. compare candidate models cand_models 

model_names <- c("d1","d2","d3")

model_compare <- AICcmodavg::aictab(cand_models, modnames = model_names)

model_compare %>%
  kbl() %>%
  kable_styling()


#8. extract d1 model and re-fit
dredge_model <- get.models(model_dredge, 1)[[1]]

dredge_fit <- lmerTest::lmer(best_model, data = model_train, REML = FALSE,
          control = lmerControl(optimizer = "bobyqa"))

summary(best_fit)

options(na.action = "na.omit")

```


##### Compare DIC from `dredge()` and `lmerTest::step()`

AIC stands for Akaike Information Criterion:

* AIC estimates the quality of a model in comparison to other models and thus can be used for model selection.
* It works by evaluating the model's fit on the training data and adding a penalty term for the complexity of the model.

*Compare and select best model from `dredge()`

```{r}

best_models <- list(lmer_final, dredge_model)

model_names <- c("step", "dredge")

all_model_compare <- AICcmodavg::aictab(best_models, modnames = model_names)

all_model_compare %>%
  kbl() %>%
  kable_styling()


r_sq_step <- r.squaredGLMM(lmer_final)
r_sq_dred <- r.squaredGLMM(best_model)


model_comparison <- rbind(r_sq_step, r_sq_dred)

model_comparison <- cbind(all_model_compare, model_comparison)

model_comparison %>%
  kbl() %>%
  kable_styling()

```




```{r}
# visualise relationship between predictors and outcomes
library(visreg)

visreg(z, xvar = "rides_per_training_phase")
visreg(z, xvar = "week_coggan_np")
visreg(z, xvar = "week_total_workout_time")

# 7. ANOVA

anova(z, type = 3)
```

##### Reporting

```{r}
install.packages("easystats", repos = "https://easystats.r-universe.dev")

remove.packages("tidymodels")

remotes::install_github("easystats/parameters")
library(parameters)
library(easystats)
library(report)



easystats::r

```

