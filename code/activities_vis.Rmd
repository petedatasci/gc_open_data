---
title: "Activities CP - Vis and Analysis"
author: "Peter Bonner"
date: "19/04/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = TRUE, warning = FALSE,
                      message = FALSE,echo = TRUE, dpi = 180,
                      fig.width = 8, fig.height = 5)


```

```{r}
library(tidyverse)
library(readr)
library(janitor)
library(readxl)
library(lubridate)
library(GGally)
library(broom)
library(caret)
library(tidymodels)
library(vip)
library(car)
library(mctest)

theme_set(theme_minimal())
options(scipen = 999)
```


# Read in activities data

Create/change variables required for analysis
```{r}
rides <- read_rds("~/R/gc_open_data/data/activities_cp.rds") %>%
  relocate(year:`30m_critical_power`,
           .after = where(is.character)) %>%
  mutate(year = as.factor(year)) %>%
  mutate(quarter = as.factor(quarter)) %>%
  mutate(cp_quantile = ntile(critical_power,4)) %>%
  mutate(pwr_hr_ratio = average_power / average_hr) %>%
  select(-X34)

```

# Training metrics

## Training metrics by week

Summarise training metrics by weekly total or weekly average.
```{r}

# Weekly training metrics
week_total_volume <- rides %>%
  group_by(id,year,quarter,week) %>%
  summarise(across(c("workout_time_min","total_distance","elevation_gain","total_work", "coggan_tss"), ~sum(.x, na.rm = TRUE),.names = "week_{.col}"),.groups = "keep") %>%
  dplyr::rename(week_total_workout_time = week_workout_time_min,
                week_total_elevation_gain = week_elevation_gain,
                week_total_tss = week_coggan_tss) %>%
  ungroup()

week_avg_int <- rides %>%
  # calculate relative session intensity
  mutate(relative_intensity = average_power / critical_power) %>%
  # calculate weekly averages
  group_by(id,year,quarter,week) %>%
  summarise(across(c(average_speed:coggan_if,relative_intensity,pwr_hr_ratio), ~ mean(.x, na.rm = TRUE),.names = "week_{.col}"),.groups = "keep") %>%
  ungroup()
  
weekly_metrics <- full_join(week_total_volume, week_avg_int, by = c("id", "year", "quarter", "week"))

```

## Training metrics by quarter

Summarise weekly averages/totals by calculating the average per quarter. E.g. average weekly training hours per 3 month period.
```{r}
quarter_metrics <- weekly_metrics %>%
  group_by(id,year,quarter) %>%
  summarise(across(c(week_total_workout_time:week_pwr_hr_ratio), ~ mean(.x, na.rm = TRUE)),.groups = "keep") %>%
  ungroup()

# join with cp metrics
cp_metrics <- rides %>%
  select(id,age,year,quarter, gender,critical_power,w_prime,critical_power_error,w_prime_error,cp_quantile) %>%
  unique()

ride_metrics <- full_join(quarter_metrics, cp_metrics, by = c("id","year","quarter")) %>%
  drop_na()

```


```{r}
# # Quarterly totals for volume measures
# quart_avg_vol <- weekly_metrics %>%
#   group_by(id,year,quarter) %>%
#   summarise(across(c("week_total_workout_time","week_total_distance","week_total_elevation_gain","week_total_work", "week_total_tss"), ~ sum(.x, na.rm = TRUE)),.groups = "keep")
# 
# # quarter averages for intensity measures
# quart_avg_int <- weekly_metrics %>%
#   group_by(id,year,quarter) %>%
#   summarise(across(c(week_average_speed:week_coggan_if,week_relative_intensity), ~ mean(.x, na.rm = TRUE)),.groups = "keep") %>%
#   ungroup()
```

## Quarterly Visuals
```{r}
# TSS
weekly_metrics %>%
  group_by(id) %>%
  summarise(average_tss = mean(week_total_tss)) %>%
  filter(average_tss >=100 & average_tss <= 1500) %>%
  ggplot(aes(x = average_tss, y = ..density..)) +
  geom_histogram(binwidth = 25, colour = "#e9ecef" ,fill = "#006c89") +
  scale_x_continuous(limits = c(100,1500),
                     breaks = seq(100,1500, by = 100)) +
  labs(
    title = "Average weekly TSS",
    x = NULL
  )

# Relative Intensity
weekly_metrics %>%
  group_by(id) %>%
  summarise(average_relative_intensity = mean(week_relative_intensity)) %>%
  ggplot(aes(x = average_relative_intensity, y = ..density..,)) +
  geom_histogram(binwidth = 0.05, colour = "#e9ecef" ,fill = "#006c89") +
  scale_x_continuous(limits = c(0.2,1.2),
                     breaks = seq(0.2,1.2, by = 0.05)) +
  labs(title = "Average weekly relative intensity",
    x = NULL)

# Hours
weekly_metrics %>%
  group_by(id) %>%
  summarise(average_weekly_hours = mean(week_total_workout_time)) %>%
  filter(average_weekly_hours <= 2400) %>% 
  ggplot(aes(x = average_weekly_hours, y = ..density..,)) +
  geom_histogram(binwidth = 30, colour = "#e9ecef" ,fill = "#006c89") +
  scale_x_continuous(limits = c(0,2400),
                     breaks = seq(0,2400, by = 100)) +
  labs(title = "Average weekly minutes of training",
    x = NULL)

```

# Linear Model

## Create a linear model
```{r}

# Create a split object
set.seed(2021)

ride_split <- ride_metrics %>%
  select(-id,-quarter,-year,-gender,-w_prime_error, -critical_power_error,-w_prime,-cp_quantile) %>%
  initial_split()

ride_training <- ride_split %>%
  training()

ride_test <- ride_split %>%
  testing()

#  -week_total_work,-week_total_distance
```


```{r}
# Create model
model <- linear_reg() %>%
  set_engine("lm") %>%  # adds lm implementation of linear regression
  set_mode("regression")

# Fitting to training data

model_fit <- model %>%
  fit(critical_power ~., data = ride_training)
```

## Evaluate model
```{r}
## View model_fit properties
model_fit

names(model_fit)

summary(model_fit$fit)

```

## Variance inflation factor
```{r}
# Cor matrix
library(corrplot)
cor1 <- cor(ride_training)
corrplot.mixed(cor1, lower.col = "black", numeber.cex = .7)

# Check multicollinearity with variance inflation factor (VIF)
vif_values <- car::vif(model_fit$fit)
vif_values

mc.plot(model_fit$fit, Inter = TRUE)

```

Based on variance inflation factor and removing variables individually based on a VIF of grater than 10, the variables `week_total_work`, `week_total_distance` were removed from the data set. (Book et al., 2001,2012,2017).

# Adapted linear model

```{r}

# Create a split object
set.seed(2021)

ride_split <- ride_metrics %>%
  select(-id,-quarter,-year,-gender,-w_prime_error, -critical_power_error,-w_prime,-cp_quantile, -week_total_work,-week_total_distance) %>%
  initial_split()

ride_training <- ride_split %>%
  training()

ride_test <- ride_split %>%
  testing()

#  -week_total_work,-week_total_distance
```


```{r}
# Create model
model <- linear_reg() %>%
  set_engine("lm") %>%  # adds lm implementation of linear regression
  set_mode("regression")

# Fitting to training data

model_fit <- model %>%
  fit(critical_power ~., data = ride_training)
```

## Evaluate model
```{r}
## View model_fit properties
model_fit

names(model_fit)

summary(model_fit$fit)

```

## Variance inflation factor
```{r}
# Cor matrix
library(corrplot)
cor1 <- cor(ride_training)
corrplot.mixed(cor1, lower.col = "black", numeber.cex = .7)

# Check multicollinearity with variance inflation factor (VIF)
vif_values <- car::vif(model_fit$fit)
vif_values

mc.plot(model_fit$fit, Inter = TRUE)

```

## Diagnostic Plots
```{r}
# Diagnostic plots

par(mfrow=c(2,2))

plot(model_fit$fit,
     pch = 16,  # optional parameters to make points blue
     col = '#006EA1')

```

Diagnostic plots:
* Q-Q plot suggests that the data have more extreme values than would be expected from data that was normally distributed.
* Residuals vs Fitted - suggests that the relationship may be non-linear (quadratic in nature?)
  * may be missing higher order variable to explain the pattern/missing variables/missing interaction between terms in the current model


```{r}
# Tidy training results

## df of estimated coefficients
tidy(model_fit)

# Performance metrics on training data
glance(model_fit)
```

## Variable Importance

```{r}
# variable importance
vip(model_fit, num_features = 15)
```

# Dimensionality reduction: Principle component analysis

```{r}
# Method 1 - tidy models (AH)
pca_df <- ride_metrics %>%
  select(-id,-year,-quarter,-critical_power,-w_prime,-critical_power_error,-w_prime_error,-week_total_work,-week_total_distance)

pca_recipe <- recipe(~ ., data = pca_df) %>%
  update_role(gender,cp_quantile, new_role = "id") %>%
  step_naomit(all_predictors()) %>%
  step_normalize(all_predictors()) %>%
  step_pca(all_predictors(), num_comp = 6, id = "pca")
  
pca_prep <- prep(pca_recipe)

cycling_pca <- pca_prep %>%
  tidy(id = "pca")

cycling_pca

```

```{r}
# bake recipe

bake(pca_prep, new_data = NULL)
```


We can also apply the `recipes::tidy()` method to the output from `recipes::step_pca()` to examine how much variance each component accounts for:

```{r}
pca_prep %>%
  tidy(id = "pca", type = "variance") %>%
  filter(terms == "percent variance") %>%
  ggplot(aes(x = component, y = value)) +
  geom_col(fill = "#b6dfe2") + 
  xlim(c(0, 10)) + 
  ylab("% of total variance")
```

We can plot these loadings by principal component too, following Julia Silgeâ€™s example:

```{r}
# Plot loadings

cycling_pca %>%
    mutate(terms = tidytext::reorder_within(terms, 
                                          abs(value), 
                                          component)) %>%
  ggplot(aes(abs(value), terms, fill = value > 0)) +
  geom_col() +
  facet_wrap(~component, scales = "free_y") +
  tidytext::scale_y_reordered() +
  scale_fill_manual(values = c("#b6dfe2", "#0A537D")) +
  labs(
    x = "Absolute value of contribution",
    y = NULL, fill = "Positive?"
  ) 


# PC1 bar chart - 10000 ft

cycling_pca %>% 
    filter(component %in% paste0("PC", 1:6)) %>%
    ggplot(aes(terms, value, fill = terms)) +
    geom_col(show.legend = FALSE, alpha = 0.8) +
    theme(axis.text.x = element_blank(), 
          axis.ticks.x = element_blank(),
          panel.grid.major = element_blank(), 
          panel.grid.minor = element_blank()) + 
    labs(x = "Ride metrics",
         y = "Relative importance in each principal component") +
    facet_wrap(~ component, ncol = 2)

# PC1 bar chart
cycling_pca %>%
  filter(component == "PC1") %>%
  top_n(14, abs(value)) %>%
  mutate(terms = reorder(terms, value)) %>%
  ggplot(aes(terms, value, fill = terms)) +
  geom_col(show.legend = FALSE, alpha = 0.8) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5),         axis.ticks.x = element_blank()) + 
  labs(x = "Ride Metrics",
       y = "Relative importance in principle component")

```

PCA1 suggests that measures of exercise intensity increase together. A high average power means a higher normalised power and relative intensity (averaged on a per week basis). An increasing average power suggests a decreasing total workout time.


```{r}
# PC2 bar chart
cycling_pca %>%
  filter(component == "PC2") %>%
  top_n(14, abs(value)) %>%
  mutate(terms = reorder(terms, value)) %>%
  ggplot(aes(terms, value, fill = terms)) +
  geom_col(show.legend = FALSE, alpha = 0.8) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5), 
        axis.ticks.x = element_blank()) + 
  labs(x = "Ride Metrics",
       y = "Relative importance in principle component")
```

PCA2 increases with both total workout time and weekly tss total, both represent how much training has been done on a weekly basis. Those who have a higher weekly training volume have a decreasing relative intensity.


```{r}
# PC3 bar chart

cycling_pca %>%
  filter(component == "PC3") %>%
  top_n(14, abs(value)) %>%
  mutate(terms = reorder(terms, value)) %>%
  ggplot(aes(terms, value, fill = terms)) +
  geom_col(show.legend = FALSE, alpha = 0.8) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5), 
        axis.ticks.x = element_blank()) + 
  labs(x = "Ride Metrics",
       y = "Relative importance in principle component")
```

PCA3 increases with an increased power to heart rate ratio and decreases with average heart rate. This suggests may be suggestive of interval training? Whereby a a higher intensity is not usually matched by higher heart rate values??


## Plot PCA loadings + scores
We have the PCA loadings in `cycling_pca`. But we need them in a wide format now for plotting.

```{r}
# get pca loadings into wider format
pca_wider <- cycling_pca %>% 
  tidyr::pivot_wider(names_from = component, id_cols = terms)
```

We also need to go back to our prepped cycling recipe, `pca_prep`, and `recipes::juice()` it to get the PCA scores back.

## PCA Plots coloured by CP Quarile
```{r}
library(ggrepel)
#PCA 1 vs PCA 2
# define arrow style
arrow_style <- arrow(
  angle = 20, length = grid::unit(8, "pt"),
  ends = "first", type = "closed"
)

pca_plot <-
  bake(pca_prep, new_data = NULL) %>%
  ggplot(aes(PC1, PC2)) +
  geom_point(aes(colour = factor(cp_quantile), shape = factor(cp_quantile)), 
             #alpha = 0.4, 
             size = 2) +
  scale_colour_manual(values = c("darkorange","purple","cyan4","midnightblue")) +
  scale_shape(solid = FALSE)

pca_plot +
  geom_segment(data = pca_wider,
               aes(xend = PC1, yend = PC2), 
               x = 0, 
               y = 0, 
               arrow = arrow_style) + 
  geom_text_repel(data = pca_wider,
            aes(x = PC1, y = PC2, label = terms), 
            xlim = c(-Inf, Inf), ylim = c(-Inf, Inf),
            size = 5, 
            color = "black") 

pca_plot

# PCA 1 vs PCA 3
pca_plot_2 <-
  bake(pca_prep, new_data = NULL) %>%
  ggplot(aes(PC1, PC3)) +
  geom_point(aes(colour = factor(cp_quantile), shape = factor(cp_quantile)), 
             #alpha = 0.4, 
             size = 2) +
  scale_colour_manual(values = c("darkorange","purple","cyan4","midnightblue")) +
  scale_shape(solid = FALSE)

pca_plot_2 +
  geom_segment(data = pca_wider,
               aes(xend = PC1, yend = PC3), 
               x = 0, 
               y = 0, 
               arrow = arrow_style) + 
  geom_text_repel(data = pca_wider,
            aes(x = PC1, y = PC3, label = terms), 
            xlim = c(-Inf, Inf), ylim = c(-Inf, Inf),
            size = 5, 
            color = "black") 

pca_plot2
```

## PCA Plots not coloured

```{r}
bake(pca_prep, new_data = NULL) %>%
  ggplot(aes(PC1, PC2)) +
  geom_point(size = 1.3, colour = "midnightblue", alpha = 0.1)
  
bake(pca_prep, new_data = NULL) %>%
  ggplot(aes(PC1, PC3)) +
  geom_point(size = 1.3, colour = "midnightblue", alpha = 0.1)
  
```

# Remove uneeded dfs

```{r}
remove(model)
remove(model_fit)
remove(rides)
remove(vif_values)
remove(rides)
remove(cp_metrics)
remove(week_avg_int)
remove(week_total_volume)
```


# Clustering

```{r}
library(dbscan)
library(factoextra)
library(NbClust)

# Create cluster df
cluster_df <- ride_metrics %>%
  select(-id,-year,-quarter,-gender,-w_prime_error,-critical_power_error,-cp_quantile,-week_total_work,-week_total_distance) %>%
  scale() %>%
  as_tibble()
```

## K-means

https://www.tidymodels.org/learn/statistics/k-means/
```{r}
# K-means

ride_clust <- kmeans(cluster_df, centers = 4)
ride_clust
summary(ride_clust)

## tidy results
tidy(ride_clust)
```

While these summaries are useful, they would not have been too difficult to extract out from the data set yourself. The real power comes from combining these analyses with other tools like dplyr.

Letâ€™s say we want to explore the effect of different choices of k, from 1 to 9, on this clustering. First cluster the data 9 times, each using a different value of k, then create columns containing the tidied, glanced and augmented data:

```{r}
ride_clusts <- 
  tibble(k = 1:9) %>%
  mutate(ride_clust = map(k, ~kmeans(cluster_df, .x)),
         tidied = map(ride_clust, tidy),
         glanced = map(ride_clust, glance),
         augmented = map(ride_clust, augment, cluster_df)
  )
```

We can turn these into three separate data sets each representing a different type of data: using tidy(), using augment(), and using glance(). Each of these goes into a separate data set as they represent different types of data.

```{r}
clusters <- 
  ride_clusts %>%
  unnest(cols = c(tidied))

assignments <- 
  ride_clusts %>% 
  unnest(cols = c(augmented))

clusterings <- 
  ride_clusts %>%
  unnest(cols = c(glanced))
```

Now we can plot the original points using the data from augment(), with each point colored according to the predicted cluster.

```{r}
# Assignments plot
ggplot(assignments, aes(x = critical_power, y = week_total_tss)) +
  geom_point(aes(color = .cluster), alpha = 0.8) + 
  facet_wrap(~ k)

```

Already we get a good sense of the proper number of clusters (3), and how the k-means algorithm functions when k is too high or too low. We can then add the centers of the cluster using the data from tidy():

```{r}
# Clusters plot

ggplot(assignments, aes(x = critical_power, y = week_average_power)) +
  geom_point(aes(color = .cluster), alpha = 0.8) + 
  facet_wrap(~ k) +
  geom_point(data = clusters, size = 10, shape = "x")
```

The clusters plot above suggests that k = 3 may be an optimal number for k.

The data from `glance()` fills a different but equally important purpose; it lets us view trends of some summary statistics across values of `k`. Of particular interest is the total within sum of squares, saved in the `tot.withinss` column.

```{r}
# Elbow plot
ggplot(clusterings, aes(k, tot.withinss)) +
  geom_line() +
  geom_point()
```

The elbow plot above also suggests that k =3 might be an optimal number for k. Therefore, we re-run the cluster analysis and use a silhouette plot to confirm. 

Si > 0 means the observation is well clustered. The closer it is to 1, the better it is clustered.
Si < 0 means the observation was placed in the wrong cluster
Si = 0 means the observation is between two clusters

```{r}
# Re-run k means with k=3
kclust <- kmeans(cluster_df, centers = 3, nstart = 20)
kclust$cluster

clusters <- 
  ride_clusts %>%
  unnest(cols = c(tidied))

assignments <- 
  ride_clusts %>% 
  unnest(cols = c(augmented))

clusterings <- 
  ride_clusts %>%
  unnest(cols = c(glanced))


sil <- cluster::silhouette(kclust$cluster, dist(cluster_df))
fviz_silhouette(sil)

# Objects with negative silhouette
neg_sil_index <- which(sil[, 'sil_width'] < 0)
sil[neg_sil_index, , drop = FALSE]


# scatter plot - how many are mis-clustered?
as_tibble(sil[neg_sil_index, , drop = FALSE]) %>%
  ggplot(aes(x = cluster, y = neighbor)) +
  geom_jitter()

```

## Clustering visualisation

```{r}
# bind cluster results into cluster dataframe
cluster_df_blend <- cbind(cluster_df, kclust = kclust$cluster, cp_quantile = ride_metrics$cp_quantile) %>%
  as_tibble()

```


```{r}
# Grid showing clusters for important variables
library(cowplot)


# total weekly hours
hours_plot <- ggplot(cluster_df_blend, aes(x = critical_power, y = week_total_workout_time)) +
  geom_point(aes(colour = as.factor(kclust), shape = as.factor(cp_quantile)),alpha = 0.6)

avgpwr_plot <- ggplot(cluster_df_blend, aes(x = critical_power, y = week_average_power)) +
  geom_point(aes(colour = as.factor(kclust), shape = as.factor(cp_quantile)),alpha = 0.6)

ri_plot <-ggplot(cluster_df_blend, aes(x = critical_power, y = week_relative_intensity)) +
  geom_point(aes(colour = as.factor(kclust), shape = as.factor(cp_quantile)),alpha = 0.6)

tss_plot <- ggplot(cluster_df_blend, aes(x = critical_power, y = week_total_tss)) +
  geom_point(aes(colour = as.factor(kclust), shape = as.factor(cp_quantile)),alpha = 0.6)

```

```{r}
grid_plot <- plot_grid(
  hours_plot + theme(legend.position="none"),
  avgpwr_plot + theme(legend.position="none"),
  ri_plot + theme(legend.position="none"),
  tss_plot + theme(legend.position="none"),
  labels = c("A","B","C","D"),
  nrow = 2
)

grid_plot

# extract legend from one of the plots
legend <- get_legend(hours_plot + theme(legend.box.margin = margin(0,0,0,12))
)

# add legend 

plot_grid(grid_plot, legend, rel_widths = c(3,.4))
```



## Clustering interpretation

```{r}
fviz_cluster(kclust, data = cluster_df)

cluster_df_blend %>%
  group_by(cluster) %>%
  summarise_all("mean")

```
Results above suggest that:
* Cluster 1: low values across all variables (are these the lower performers?)
* Cluster 2: High value for `week_total_workout_time` and `week_total_tss` with lower values for intensity based measures - Highest critical power value
* Cluster 3: High value for `week_coggan_np`, `week_coggan_if` and `week_relative_intensity`. Essentially opposite to cluster 2.



```{r}
# bind clusters into original data frame
metrics_blend <- cbind(ride_metrics, kclust$cluster) %>%
  as_tibble()

  
```





# IGNORE

## Training metrics vs CP
```{r}
# Training duration
quarter_metrics %>%
  filter(total_workout_time <= 1000000) %>%
  ggplot(aes(x = total_workout_time, y = critical_power)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm",colour = "blue")

# Training distance
quarter_metrics %>%
  filter(total_total_distance >0 & total_total_distance <= 20000) %>%
  ggplot(aes(x = total_total_distance, y = critical_power)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm",colour = "blue")

# Average power 
quarter_metrics %>%
    ggplot(aes(x = average_average_power, y = critical_power)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = "lm",colour = "blue")

# Average HR
quarter_metrics %>%
  filter(average_average_hr > 50) %>%
  ggplot(aes(x = average_average_hr, y = critical_power)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = "lm",colour = "blue")
```

```{r}

```





## Average change in CP over time

```{r}
cp_stats <- activities_cp
# calculate % change in cp/w_prime for each quarter per person
  group_by(id) %>%
  arrange(year, quarter, .by_group = TRUE) %>%
  mutate(cp_pct_change = (critical_power / lag(critical_power)-1)*100,
         w_prime_pct_change = (w_prime / lag(w_prime)-1)*100)

# change in cp per quarter
cp_stats %>%
  filter(year > 1999 & year <= 2021) %>%
  group_by(year) %>%
  summarise(avg_change = mean(cp_pct_change, na.rm = TRUE)) %>%
  arrange(avg_change) %>%
  ggplot(aes(x = factor(year), y = avg_change)) +
  geom_col(fill = "#f03b20", alpha = 0.5) +
  labs(title = "Average percent change in critical power per quarter",
       x = NULL,
       y = NULL)

# change in wprime per quarter
cp_stats %>%
  filter(year > 1999 & year <= 2021) %>%
  group_by(year) %>%
  summarise(avg_change = mean(w_prime_pct_change, na.rm = TRUE)) %>%
  arrange(avg_change) %>%
  ggplot(aes(x = factor(year), y = avg_change)) +
  geom_col(fill = "#f03b20", alpha = 0.5) +
  labs(title = "Average percent change in W' per quarter",
       x = NULL,
       y = NULL)

```