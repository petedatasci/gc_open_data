---
title: "Activities CP - Vis and Analysis"
author: "Peter Bonner"
date: "19/04/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = TRUE, warning = FALSE,
                      message = FALSE,echo = TRUE, dpi = 180,
                      fig.width = 8, fig.height = 5)


```

```{r}
library(tidyverse)
library(readr)
library(janitor)
library(readxl)
library(lubridate)
library(GGally)
library(broom)
library(caret)
library(tidymodels)
library(vip)
library(car)
library(mctest)

theme_set(theme_minimal())
options(scipen = 999)
```


## Read in activities data
```{r}
rides <- read_rds("~/R/gc_open_data/data/activities_cp.rds") %>%
  relocate(year:`30m_critical_power`,
           .after = where(is.character)) %>%
  mutate(year = as.factor(year)) %>%
  mutate(pwr_hr_ratio = average_power / average_hr) %>%
  select(-X34)


```


## Training metrics by week

Summarise training metrics by weekly total or weekly average.
```{r}

# Weekly training metrics
week_total_volume <- rides %>%
  group_by(id,year,quarter,week) %>%
  summarise(across(c("workout_time_min","total_distance","elevation_gain","total_work", "coggan_tss"), ~sum(.x, na.rm = TRUE),.names = "week_{.col}"),.groups = "keep") %>%
  dplyr::rename(week_total_workout_time = week_workout_time_min,
                week_total_elevation_gain = week_elevation_gain,
                week_total_tss = week_coggan_tss) %>%
  ungroup()

week_avg_int <- rides %>%
  # calculate relative session intensity
  mutate(relative_intensity = average_power / critical_power) %>%
  # calculate weekly averages
  group_by(id,year,quarter,week) %>%
  summarise(across(c(average_speed:coggan_if,relative_intensity,pwr_hr_ratio), ~ mean(.x, na.rm = TRUE),.names = "week_{.col}"),.groups = "keep") %>%
  ungroup()
  
weekly_metrics <-full_join(week_total_volume, week_avg_int, by = c("id", "year", "quarter", "week"))

```

## Training metrics by quarter

Summarise weekly averages/totals by calculating the average per quarter. E.g. average weekly training hours per 3 month period.
```{r}
quarter_metrics <- weekly_metrics %>%
  group_by(id,year,quarter) %>%
  summarise(across(c(week_total_workout_time:week_pwr_hr_ratio), ~ mean(.x, na.rm = TRUE)),.groups = "keep") %>%
  ungroup()

# join with cp metrics
cp_metrics <- rides %>%
  select(id,age,year,quarter, gender,critical_power,w_prime,critical_power_error,w_prime_error) %>%
  unique()

ride_metrics <- full_join(quarter_metrics, cp_metrics, by = c("id","year","quarter"))


```


```{r}
# # Quarterly totals for volume measures
# quart_avg_vol <- weekly_metrics %>%
#   group_by(id,year,quarter) %>%
#   summarise(across(c("week_total_workout_time","week_total_distance","week_total_elevation_gain","week_total_work", "week_total_tss"), ~ sum(.x, na.rm = TRUE)),.groups = "keep")
# 
# # quarter averages for intensity measures
# quart_avg_int <- weekly_metrics %>%
#   group_by(id,year,quarter) %>%
#   summarise(across(c(week_average_speed:week_coggan_if,week_relative_intensity), ~ mean(.x, na.rm = TRUE)),.groups = "keep") %>%
#   ungroup()
```

## Quarterly Visuals
```{r}
# TSS
weekly_metrics %>%
  group_by(id) %>%
  summarise(average_tss = mean(week_total_tss)) %>%
  filter(average_tss >=100 & average_tss <= 1500) %>%
  ggplot(aes(x = average_tss, y = ..density..)) +
  geom_histogram(binwidth = 25, colour = "#e9ecef" ,fill = "#006c89") +
  scale_x_continuous(limits = c(100,1500),
                     breaks = seq(100,1500, by = 100)) +
  labs(
    title = "Average weekly TSS",
    x = NULL
  )

# Relative Intensity
weekly_metrics %>%
  group_by(id) %>%
  summarise(average_relative_intensity = mean(week_relative_intensity)) %>%
  ggplot(aes(x = average_relative_intensity, y = ..density..,)) +
  geom_histogram(binwidth = 0.05, colour = "#e9ecef" ,fill = "#006c89") +
  scale_x_continuous(limits = c(0.2,1.2),
                     breaks = seq(0.2,1.2, by = 0.05)) +
  labs(title = "Average weekly relative intensity",
    x = NULL)

# Hours
weekly_metrics %>%
  group_by(id) %>%
  summarise(average_weekly_hours = mean(week_total_workout_time)) %>%
  filter(average_weekly_hours <= 2400) %>% 
  ggplot(aes(x = average_weekly_hours, y = ..density..,)) +
  geom_histogram(binwidth = 30, colour = "#e9ecef" ,fill = "#006c89") +
  scale_x_continuous(limits = c(0,2400),
                     breaks = seq(0,2400, by = 100)) +
  labs(title = "Average weekly minutes of training",
    x = NULL)

```

## Create a linear model
```{r}

# Create a split object
set.seed(2021)

ride_split <- ride_metrics %>%
  select(-id,-quarter,-year,-gender,-w_prime_error, -critical_power_error,-w_prime, -week_total_work,-week_total_distance) %>%
  initial_split()

ride_training <- ride_split %>%
  training()

ride_test <- ride_split %>%
  testing()
```

`week_total_work` and `week_total_distance` removed due to collinearity: Variance inflation factor > 10 is problematic (Book et al., 2001,2012,2017).

```{r}
# Create model
model <- linear_reg() %>%
  set_engine("lm") %>%  # adds lm implementation of linear regression
  set_mode("regression")

# Fitting to training data

model_fit <- model %>%
  fit(critical_power ~., data = ride_training)
```

## Evaluate model
```{r}
## View model_fit properties
model_fit

names(model_fit)

summary(model_fit$fit)

```

```{r}
# Cor matrix
library(corrplot)
cor1 <- cor(ride_training)
corrplot.mixed(cor1, lower.col = "black", numeber.cex = .7)

# Check multicollinearity with variance inflation factor (VIF)
vif_values <- car::vif(model_fit$fit)
vif_values

mc.plot(model_fit$fit, Inter = TRUE)

```

```{r}
# Diagnostic plots

par(mfrow=c(2,2))

plot(model_fit$fit,
     pch = 16,  # optional parameters to make points blue
     col = '#006EA1')

```

Diagnostic plots:
* Q-Q plot suggests that the data have more extreme values than would be expected from data that was normally distributed.
* Residuals vs Fitted - suggests that the relationship may be non-linear (quadratic in nature?)
  * may be missing higher order variable to explain the pattern/missing variables/missing interaction between terms in the current model


```{r}
# Tidy training results

## df of estimated coefficients
tidy(model_fit)

# Performance metrics on training data
glance(model_fit)
```

```{r}
# variable importance
vip(model_fit, num_features = 15)
```



# Dimensionality reduction: Principle component analysis

```{r}
# Method 1 - tidy models (AH)
pca_df <- ride_metrics %>%
  mutate(quarter = as.factor(quarter)) %>%
  mutate(cp_quantile = ntile(critical_power,4)) %>%
  select(-id,-year,-quarter,-critical_power,-w_prime,-critical_power_error,-w_prime_error,-week_total_work,-week_total_distance)

pca_recipe <- recipe(~ ., data = pca_df) %>%
  update_role(gender,cp_quantile, new_role = "id") %>%
  step_naomit(all_predictors()) %>%
  step_normalize(all_predictors()) %>%
  step_pca(all_predictors(), num_comp = 6, id = "pca")
  
pca_prep <- prep(pca_recipe)

cycling_pca <- pca_prep %>%
  tidy(id = "pca")

cycling_pca

```

```{r}
# bake recipe

bake(pca_prep, new_data = NULL)
```


We can also apply the `recipes::tidy()` method to the output from `recipes::step_pca()` to examine how much variance each component accounts for:

```{r}
pca_prep %>%
  tidy(id = "pca", type = "variance") %>%
  filter(terms == "percent variance") %>%
  ggplot(aes(x = component, y = value)) +
  geom_col(fill = "#b6dfe2") + 
  xlim(c(0, 10)) + 
  ylab("% of total variance")
```

We can plot these loadings by principal component too, following Julia Silgeâ€™s example:

```{r}
# Plot loadings

cycling_pca %>%
    mutate(terms = tidytext::reorder_within(terms, 
                                          abs(value), 
                                          component)) %>%
  ggplot(aes(abs(value), terms, fill = value > 0)) +
  geom_col() +
  facet_wrap(~component, scales = "free_y") +
  tidytext::scale_y_reordered() +
  scale_fill_manual(values = c("#b6dfe2", "#0A537D")) +
  labs(
    x = "Absolute value of contribution",
    y = NULL, fill = "Positive?"
  ) 


# PC1 bar chart - 10000 ft

cycling_pca %>% 
    filter(component %in% paste0("PC", 1:6)) %>%
    ggplot(aes(terms, value, fill = terms)) +
    geom_col(show.legend = FALSE, alpha = 0.8) +
    theme(axis.text.x = element_blank(), 
          axis.ticks.x = element_blank(),
          panel.grid.major = element_blank(), 
          panel.grid.minor = element_blank()) + 
    labs(x = "Ride metrics",
         y = "Relative importance in each principal component") +
    facet_wrap(~ component, ncol = 2)

# PC1 bar chart
cycling_pca %>%
  filter(component == "PC1") %>%
  top_n(14, abs(value)) %>%
  mutate(terms = reorder(terms, value)) %>%
  ggplot(aes(terms, value, fill = terms)) +
  geom_col(show.legend = FALSE, alpha = 0.8) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5), 
        axis.ticks.x = element_blank()) + 
  labs(x = "Ride Metrics",
       y = "Relative importance in principle component")

```

```{r}
# PC2 bar chart
cycling_pca %>%
  filter(component == "PC2") %>%
  top_n(14, abs(value)) %>%
  mutate(terms = reorder(terms, value)) %>%
  ggplot(aes(terms, value, fill = terms)) +
  geom_col(show.legend = FALSE, alpha = 0.8) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5), 
        axis.ticks.x = element_blank()) + 
  labs(x = "Ride Metrics",
       y = "Relative importance in principle component")
```




### Plot PCA loadings + scores
We have the PCA loadings in `cycling_pca`. But we need them in a wide format now for plotting.

```{r}
# get pca loadings into wider format
pca_wider <- cycling_pca %>% 
  tidyr::pivot_wider(names_from = component, id_cols = terms)
```

We also need to go back to our prepped cycling recipe, `pca_prep`, and `recipes::juice()` it to get the PCA scores back.

## PCA Plots coloured by CP Quarile
```{r}
library(ggrepel)
#PCA 1 vs PCA 2
# define arrow style
arrow_style <- arrow(
  angle = 20, length = grid::unit(8, "pt"),
  ends = "first", type = "closed"
)

pca_plot <-
  bake(pca_prep, new_data = NULL) %>%
  ggplot(aes(PC1, PC2)) +
  geom_point(aes(colour = factor(cp_quantile), shape = factor(cp_quantile)), 
             #alpha = 0.4, 
             size = 2) +
  scale_colour_manual(values = c("darkorange","purple","cyan4","midnightblue")) +
  scale_shape(solid = FALSE)

pca_plot +
  geom_segment(data = pca_wider,
               aes(xend = PC1, yend = PC2), 
               x = 0, 
               y = 0, 
               arrow = arrow_style) + 
  geom_text_repel(data = pca_wider,
            aes(x = PC1, y = PC2, label = terms), 
            xlim = c(-Inf, Inf), ylim = c(-Inf, Inf),
            size = 5, 
            color = "black") 

pca_plot

# PCA 1 vs PCA 3
pca_plot_2 <-
  bake(pca_prep, new_data = NULL) %>%
  ggplot(aes(PC1, PC3)) +
  geom_point(aes(colour = factor(cp_quantile), shape = factor(cp_quantile)), 
             #alpha = 0.4, 
             size = 2) +
  scale_colour_manual(values = c("darkorange","purple","cyan4","midnightblue")) +
  scale_shape(solid = FALSE)

pca_plot_2 +
  geom_segment(data = pca_wider,
               aes(xend = PC1, yend = PC3), 
               x = 0, 
               y = 0, 
               arrow = arrow_style) + 
  geom_text_repel(data = pca_wider,
            aes(x = PC1, y = PC3, label = terms), 
            xlim = c(-Inf, Inf), ylim = c(-Inf, Inf),
            size = 5, 
            color = "black") 

pca_plot2
```

## PCA Plots not coloured

```{r}
bake(pca_prep, new_data = NULL) %>%
  ggplot(aes(PC1, PC2)) +
  geom_point(size = 1.3, colour = "midnightblue", alpha = 0.1)
  
bake(pca_prep, new_data = NULL) %>%
  ggplot(aes(PC1, PC3)) +
  geom_point(size = 1.3, colour = "midnightblue", alpha = 0.1)
  
```


# Clustering

DBscan:
* eps: Reachability maximum distance
* MinPts: Reachability minimum number of points
* scale: If TRUE, the data will be scaled
* method: Possible values are:
  * dist: Treats the data as distance matrix
  * raw: Treats the data as raw data
  * hybrid: Expect also raw data, but calculates partial distance matrices 

```{r}
library(dbscan)
library(factoextra)
library(NbClust)

# Create cluster df
cluster_df <- ride_metrics %>%
  select(-id,-year,-quarter,-gender,-w_prime_error,-critical_power_error,-week_total_work,-week_total_distance) %>%
  drop_na()

fviz_nbclust(cluster_df, kmeans, method = "wss")
  labs(subtitle = "Elbow method")

# Determine DBSCAN eps value
kNNdistplot(cluster_df, k = 4)
abline(h = 10000, lty = 2)

# Determine minPts value
log(17992)

# Compute DBSCAN
set.seed(2021)
db <- dbscan(cluster_df, eps = 10000, minPts = 4)

# Plot DBSCAN results
print(db)
plot(db, cluster_df, main = "DBSCAN",frame = FALSE)
```







# Training metrics vs CP
```{r}
# Training duration
quarter_metrics %>%
  filter(total_workout_time <= 1000000) %>%
  ggplot(aes(x = total_workout_time, y = critical_power)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm",colour = "blue")

# Training distance
quarter_metrics %>%
  filter(total_total_distance >0 & total_total_distance <= 20000) %>%
  ggplot(aes(x = total_total_distance, y = critical_power)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm",colour = "blue")

# Average power 
quarter_metrics %>%
    ggplot(aes(x = average_average_power, y = critical_power)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = "lm",colour = "blue")

# Average HR
quarter_metrics %>%
  filter(average_average_hr > 50) %>%
  ggplot(aes(x = average_average_hr, y = critical_power)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = "lm",colour = "blue")
```

```{r}

```





## Average change in CP over time

```{r}
cp_stats <- activities_cp
# calculate % change in cp/w_prime for each quarter per person
  group_by(id) %>%
  arrange(year, quarter, .by_group = TRUE) %>%
  mutate(cp_pct_change = (critical_power / lag(critical_power)-1)*100,
         w_prime_pct_change = (w_prime / lag(w_prime)-1)*100)

# change in cp per quarter
cp_stats %>%
  filter(year > 1999 & year <= 2021) %>%
  group_by(year) %>%
  summarise(avg_change = mean(cp_pct_change, na.rm = TRUE)) %>%
  arrange(avg_change) %>%
  ggplot(aes(x = factor(year), y = avg_change)) +
  geom_col(fill = "#f03b20", alpha = 0.5) +
  labs(title = "Average percent change in critical power per quarter",
       x = NULL,
       y = NULL)

# change in wprime per quarter
cp_stats %>%
  filter(year > 1999 & year <= 2021) %>%
  group_by(year) %>%
  summarise(avg_change = mean(w_prime_pct_change, na.rm = TRUE)) %>%
  arrange(avg_change) %>%
  ggplot(aes(x = factor(year), y = avg_change)) +
  geom_col(fill = "#f03b20", alpha = 0.5) +
  labs(title = "Average percent change in W' per quarter",
       x = NULL,
       y = NULL)

```